In this mini project, I utilized Azure Data Factory to develop a pipeline that transfers incremental data from an Azure SQL Database table to Azure Data Lake storage. The process involves several key steps:
1.	Setting up the Data Lake Storage account: Prepare a storage solution to maintain the watermark value, which is crucial for tracking data updates.
2.	Creating Azure SQL Database
3.	Creating a Data Factory.
4.	Creating Linked Services.
5.	Configuring Datasets.
6.	Building a Pipeline.
7.	Executing the Pipeline: Understand how to initiate the pipeline operation.
8.	Monitoring the Process: Learn how to track the pipeline's performance and
Please see the project file for more detail 

